{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "First NLP Analysis.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deutschkihun/KaggleStudy/blob/master/First_NLP_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBBop-pdwDiX",
        "colab_type": "text"
      },
      "source": [
        "#### <div align='Center'><font size=\"7\" color=\"#51ABDA\">First NLP Analysis</font></div>\n",
        "\n",
        "<div align='left'><font size=\"5\" color=\"#51ABDA\">Introduction</font></div>\n",
        "This is my first NLP Analysis kernal.It was really hard journey to finish it. But I'm very proud of myself.\n",
        "<hr>\n",
        "\n",
        "\n",
        "<div align='left'><font size=\"5\" color=\"#51ABDA\">Goal</font></div>\n",
        "prediction of `selected_text` into submission file by modeling\n",
        "\n",
        "# Structure  <a class=\"anchor\" id=\"toc\"></a>\n",
        "\n",
        "* <a href=\"#sec1\">1. Preliminary step</a>\n",
        " * <a href=\"#sec1.1\"> 1.1. Jaccard score </a>\n",
        " * <a href=\"#sec1.2\"> 1.2. Importing prerequisite libraries</a>\n",
        " * <a href=\"#sec1.3\"> 1.3. Reading datasets</a>\n",
        " \n",
        "\n",
        "* <a href=\"#sec2\">2. Exploratory Data Analysis(EDA)</a>\n",
        "  * <a href=\"#sec2.1\"> 2.1. Missing values </a>\n",
        "  * <a href=\"#sec2.2\"> 2.2. Incorrect datatype</a>\n",
        "  * <a href=\"#sec2.3\"> 2.3. Sampling of Sentiment</a>\n",
        "  * <a href=\"#sec2.4\"> 2.4. Data Visualization</a>\n",
        "    * <a href=\"#sec2.4.1\"> sentiment proportion</a>   \n",
        "    * <a href=\"#sec2.4.2\"> Add new feature</a> \n",
        "    * <a href=\"#sec2.4.3\"> Text data analysis</a>   \n",
        "    \n",
        "    \n",
        "* <a href=\"#sec3\"> 3. Text data preprocessing</a>\n",
        "  * <a href=\"#sec3.1\"> 3.1. Stemming</a>\n",
        "  * <a href=\"#sec3.2\"> 3.2. Cotraction mapping</a>\n",
        "  * <a href=\"#sec3.3\"> 3.3. Spelling corrector</a>\n",
        "  * <a href=\"#sec3.4\"> 3.4. Cleanig corpus</a>\n",
        "  * <a href=\"#sec3.5\"> 3.5. Tokenization</a>\n",
        "  * <a href=\"#sec3.6\"> 3.6. Removing stopwords</a>\n",
        "\n",
        "* <a href=\"#sec4\"> 4. Word Analysis</a>\n",
        "  * <a href=\"#sec4.1\"> 4.1. Word Frequency</a>\n",
        "  * <a href=\"#sec4.2\"> 4.2. Word cloud</a> \n",
        "\n",
        "* <a href=\"#sec5\"> 5. N-gram Analysis</a>\n",
        " * <a href=\"#sec5.1\"> 5.1. bigrams </a> \n",
        " * <a href=\"#sec5.2\"> 5.2. trigrams </a> \n",
        "\n",
        "\n",
        "* <a href=\"#sec6\">6. PLUS</a>\n",
        " * <a href=\"#sec6.1\"> 6.1. Spelling corector</a>\n",
        " * <a href=\"#sec6.2\"> 6.2. Removing Weird Space</a>\n",
        "* <a href=\"#sec7\">7. Closing</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Jim8SeYwDiZ",
        "colab_type": "text"
      },
      "source": [
        "<a id='sec1'></a>\n",
        "# 1. Preliminary step\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "<a id='sec1.1'></a>\n",
        "## 1.1 Jaccard score\n",
        "\n",
        "I got a lof of helps about `Jaccard score` from [Parul's kernal](https://www.kaggle.com/parulpandey/eda-and-preprocessing-for-bert). Be sure to check out !!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "f50hrSGMwDia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Numeric \n",
        "from math import*\n",
        " \n",
        "def jaccard_numeric_values(x,y):\n",
        " \n",
        "    intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
        "    union_cardinality = len(set.union(*[set(x), set(y)]))\n",
        "    return intersection_cardinality/float(union_cardinality)\n",
        " \n",
        "jaccard_numeric_values([0,1,2,5,6],[0,2,3,5,7,9])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "rJbltkA_wDif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard_string(str1,str2):\n",
        "    \n",
        "    x = set(str1.lower().split())\n",
        "    y = set(str2.lower().split())\n",
        "    # convert String with lowercase and then split it\n",
        "    \n",
        "    z = x.intersection(y)\n",
        "    t = x.union(y)\n",
        "    # Alternative  : instead of t = x.union(y) you can write (len(x) + len(y) - len(z))\n",
        "    return float(len(z)) / float(len(t))\n",
        "\n",
        "\n",
        "Sentence_1 = 'I am a data scientist at Google'\n",
        "Sentence_2 = 'I am a software engineer at Google'\n",
        "Sentence_3 = 'I am a software engineer at Microsoft'\n",
        "\n",
        "    \n",
        "print(jaccard_string(Sentence_1,Sentence_2))\n",
        "print(jaccard_string(Sentence_1,Sentence_3))\n",
        "print(jaccard_string(Sentence_2,Sentence_3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpF0CKTDwDii",
        "colab_type": "text"
      },
      "source": [
        "<a id='sec1.2'></a>\n",
        "## 1.2 Importing prerequisite libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_Mfp9-IywDij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = 'all'\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# matplotlib and seaborn for plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# Suppress warnings \n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "EKE_PWa1wDin",
        "colab_type": "text"
      },
      "source": [
        "<a id='sec1.3'></a>\n",
        "## 1.3 Reading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nSudYKUZwDin",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
        "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
        "submission=pd.read_csv('../input/tweet-sentiment-extraction/sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_xfj5ypiwDir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First 5 rows of trains data\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_TZ85QPuwDiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape of train data\n",
        "rows = train.shape[0]\n",
        "columns = train.shape[1]\n",
        "print('The number of rows in train data are {0} and columns are {1}'.format(rows,columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_bnXz0bZwDiy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First 5 rows of test data\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nRjDxXTswDi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# shape of train data\n",
        "rows = test.shape[0]\n",
        "columns = test.shape[1]\n",
        "print('The number of rows in train data are {0} and columns are {1}'.format(rows,columns))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziVKnlZMwDi5",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2\"></a>\n",
        "# 2. Exploratory Data Analysis (EDA)\n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEQx7RQ0wDi6",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.1\"></a>\n",
        "### 2.1 Missing Value & Duplication"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Q5hCt8VdwDi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.isnull().sum()\n",
        "# checking null value in train data\n",
        "test.isnull().sum()\n",
        "# checking null value in test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7p8BYeSwDi-",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> We have one null value in train data. We need to drop out it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "7yMtmMU3wDi_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.dropna(axis = 0, how ='any',inplace=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "E2065UBYwDjC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The number of duplicated train data is:',sum(train.duplicated()))\n",
        "print('The number of duplicated test data is:',sum(test.duplicated()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlF4rO87wDjF",
        "colab_type": "text"
      },
      "source": [
        "> Observation \n",
        "\n",
        "No Duplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtNGkvGpwDjG",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.2\"></a>\n",
        "### 2.2 Incorrect datatype"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4t5iUEocwDjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9aBgfj6wDjK",
        "colab_type": "text"
      },
      "source": [
        "Observtion\n",
        "\n",
        "> right datatype : Object(String)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pmd40VAwDjK",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.3\"></a>\n",
        "### 2.3 Sampling of sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RvMnTL2MwDjL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('sample of positive sentiment: ',train[train['sentiment']=='positive']['selected_text'].sample())\n",
        "print('sample of negative sentiment: ',train[train['sentiment']=='negative']['selected_text'].sample())\n",
        "print('sample of neutral sentiment: ',train[train['sentiment']=='neutral']['selected_text'].sample())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTin659TwDjO",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.4\"></a>\n",
        "### 2.4 Data Visualization \n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUTlVtb3wDjS",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.4.1\"></a>\n",
        "### Sentiment proportion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "P5hD_ksZwDjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pie charts \n",
        "plt.figure(figsize=(17,7)) \n",
        "sorted_counts = train['sentiment'].value_counts()\n",
        "ax=plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
        "        counterclock = False,pctdistance=0.8 ,wedgeprops = {'width' : 0.4}, autopct='%1.0f%%');\n",
        "\n",
        "plt.title('Train data sentiment proportion',fontsize=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On7F_lWvwDjW",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> **Neutral : 40% > Positive : 31% > Negative : 28%**. Accroding to the train data 40% of selected text are not allocted into either positve or negative.That means there are a lot of cases which are not clear to judge whether Sentiment is expressed or not"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "0mntWxVxwDjW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pie charts \n",
        "plt.figure(figsize=(17,7)) \n",
        "sorted_counts = test['sentiment'].value_counts()\n",
        "ax=plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n",
        "        counterclock = False,pctdistance=0.8 ,wedgeprops = {'width' : 0.4}, autopct='%1.0f%%');\n",
        "\n",
        "plt.title('Test data sentiment proportion',fontsize=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIPlxj2owDjZ",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> Same as train data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQsLknIWwDjZ",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.4.2\"></a>\n",
        "### Add new feature \n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "As we know our goal is that we predict selected_text in submission file. So it's more meaningful that we add new feature on the dataframe.\n",
        "\n",
        "\n",
        "### First : Jaccard score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SSoAkwFBwDja",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def jaccard_string(str1,str2):\n",
        "    \n",
        "    x = set(str1.lower().split())\n",
        "    y = set(str2.lower().split())\n",
        "    \n",
        "    z = x.intersection(y)\n",
        "    t = x.union(y)\n",
        "    return float(len(z)) / float(len(t))\n",
        "\n",
        "# Just remember the beginning of this kernel. I introduced the jaccard score with python code. Copy it !!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kPwLBOL0wDje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jaccard_score = []\n",
        "\n",
        "for rows in train.itertuples():\n",
        "    # another option : for index,rows in iterrows(), but in this case itertuples is faster than iterrows\n",
        "    # Iterate over DataFrame rows as namedtuples.\n",
        "    sentence1 = rows.text\n",
        "    # first we saved text column as sentence 1 \n",
        "    sentence2 = rows.selected_text\n",
        "    # second we saved selected_text column as sentence 2\n",
        "\n",
        "    \n",
        "    jaccard_result = jaccard_string(sentence1,sentence2)\n",
        "    # Now we apply jaccard score algoritum in sentnce 1,2\n",
        "\n",
        "    jaccard_score.append([sentence1,sentence2,jaccard_result])\n",
        "    # append sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "9ktvuzqVwDjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jaccard = pd.DataFrame(jaccard_score,columns=['text','selected_text','jaccard score'])\n",
        "# save in dataframe"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "A5cL_b1rwDjk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "jaccard.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "LP8YEXGbwDjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train=train.merge(jaccard,how='outer')\n",
        "train.head()\n",
        "\n",
        "# jaccard dataframe is merged with train data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwmB-fkEwDjp",
        "colab_type": "text"
      },
      "source": [
        "### Second : number of word and difference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gd_yuiJzwDjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['number of word in T'] = train['text'].apply(lambda x : len(str(x).split() ))   # number of word in Text\n",
        "train['number of word in ST'] = train['selected_text'].apply(lambda x : len(str(x).split() )) # number of word in Selected_text\n",
        "\n",
        "train['difference'] = train['number of word in T'] - train['number of word in ST']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "4IUG8kgrwDjt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q0zTgy8wDjw",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "finally we have jaccard and difference in number of word between `text` and `selected_text`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2ALw1nYwDjx",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec2.4.3\"></a>\n",
        "### Text data analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QCVq9V1NwDjx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive = train[train['sentiment']=='positive']\n",
        "negative = train[train['sentiment']=='negative']\n",
        "neutral = train[train['sentiment']=='neutral']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEMeq0UHwDj0",
        "colab_type": "text"
      },
      "source": [
        "**First : Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "GYr5oTzJwDj0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in T',bins=33,data=train,edgecolor='black',color='red')\n",
        "plt.title('Number of word in text',fontsize=20)\n",
        "plt.xlabel('Number of word in text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6n8yODRTwDj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in T',bins=33,data=positive,edgecolor='black',color='red')\n",
        "plt.title('Positive sentiment word in text',fontsize=20)\n",
        "plt.xlabel('number of word in text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "-MOp_m0YwDj4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in T',bins=33,data=negative,edgecolor='black',color='red')\n",
        "plt.title('Negative sentiment word in text',fontsize=20)\n",
        "plt.xlabel('number of word in text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ZzdPsUUTwDj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in T',bins=33,data=neutral,edgecolor='black',color='red')\n",
        "plt.title('Neutral sentiment word in text',fontsize=20)\n",
        "plt.xlabel('number of word in text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU3XG_jnwDj-",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "\n",
        "> Distribution without sentiment filtering and with filtering are showing very similar tendecy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "jsHRpc8_wDj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in ST',bins=33,data=train,edgecolor='black',color='green')\n",
        "plt.title('Number of word in selected text',fontsize=20)\n",
        "plt.xlabel('Number of word in selected text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qb59fS-UwDkA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in ST',bins=33,data=positive,edgecolor='black',color='green')\n",
        "plt.title('Positive sentiment word in selected text',fontsize=20)\n",
        "plt.xlabel('Number of word in selected text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nGQSof7TwDkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in ST',bins=33,data=negative,edgecolor='black',color='green')\n",
        "plt.title('Negative sentiment word in selected text',fontsize=20)\n",
        "plt.xlabel('Number of word in selected text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "OImoU8bcwDkE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(15,8))\n",
        "plt.hist(x='number of word in ST',bins=33,data=neutral,edgecolor='black',color='green')\n",
        "plt.title('Neutral sentiment word in selected text',fontsize=20)\n",
        "plt.xlabel('Number of word in selected text')\n",
        "plt.ylabel('Counting')\n",
        "x1 = list(range(0,33,1))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvjE1CkBwDkF",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> Unlike text column, selected text has one exception. Without neutral sentiment graphs represent strong right skewed distribution.But neutral sentiment is showing different shape which is almost same tendency like text column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "KPQT8yJ5wDkG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "p1=sns.kdeplot(train['number of word in ST'], shade=True, color=\"r\")\n",
        "p1.set_title('Distribution of Number Of words',fontsize=20)\n",
        "p1=sns.kdeplot(train['number of word in T'], shade=True, color=\"b\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jNchOgTwDkH",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "\n",
        "> Remarkably different distribution of `number of word from text` and `number of word from selectec text`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "_2wj34arwDkI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,8))\n",
        "p1=sns.kdeplot(train['difference'], shade=True, color=\"y\")\n",
        "p1.set_title('Distribution of Difference',fontsize=20)\n",
        "x1 = list(range(0,35,3))\n",
        "plt.xticks(x1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtFu6pMUwDkK",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> about 20% of word difference is zero.That mean 20% of selected text are 100% same as text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "b2pIe_r3wDkL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20,8))\n",
        "p1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard score'], shade=True, color=\"r\").set_title('Distribution of Jaccrd score',fontsize=20)\n",
        "p1=sns.kdeplot(train[train['sentiment']=='negative']['jaccard score'], shade=True, color=\"b\")\n",
        "plt.legend(['positive sentiment','negative sentiment'],title='Jaccard score');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "p69_xSw-wDkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(15,15))\n",
        "g=sns.jointplot(\"number of word in T\", \"number of word in ST\", data=train,kind=\"kde\", space=0, color=\"g\");\n",
        "g.fig.suptitle(\"number of word in Text and selected Text\",fontsize=20)\n",
        "\n",
        "# Format nicely.\n",
        "g.fig.tight_layout()\n",
        "\n",
        "#Reduce plot to make room for suptitle\n",
        "g.fig.subplots_adjust(top=0.9)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiSYDsEYwDkO",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3\"></a>\n",
        "# 3. Text data preprocessing\n",
        "\n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROXe1so3wDkO",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3.1\"></a>\n",
        "## 3.1 Stemming\n",
        "\n",
        "stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\n",
        "\n",
        "Like here\n",
        "\n",
        "- sleeping -> sleep\n",
        "- sleepy -> sleep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RZRkpELRwDkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "def stemming(words):\n",
        "    s = SnowballStemmer('english')\n",
        "    s.stem(words)\n",
        "    return words "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ur0wKPjiwDkR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['text'] = train['text'].apply(lambda x: stemming(x))\n",
        "test['text'] = test['text'].apply(lambda x:stemming(x))\n",
        "train['selected_text'] = train['selected_text'].apply(lambda x: stemming(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtA5WkKFwDkV",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3.2\"></a>\n",
        "## 3.2 Contraction mapping\n",
        "\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "Contraction are shortened version of words or syllables. By nature, contraction pose a problem for NLP and text analysis because we have a special apostrophe character in the word. Hence, there should be some definitve process by which we can deal with contractions when processing text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "gw88yKczwDkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Expanding contraction \n",
        "\"\"\"\n",
        "dic = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
        "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
        "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
        "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
        "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
        "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
        "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
        "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
        "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
        "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
        "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
        "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
        "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
        "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
        "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
        "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
        "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
        "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
        "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
        "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
        "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
        "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
        "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
        "                   \"this's\": \"this is\",\n",
        "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
        "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
        "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
        "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
        "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
        "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
        "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
        "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
        "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
        "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
        "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
        "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
        "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
        "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
        "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
        "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
        "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" };"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "E3_udFP3wDkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mapping_replacer(x, dic): \n",
        "    for word in dic.keys(): \n",
        "        if \" \" + word + \" \" in x: \n",
        "            x = x.replace(\" \" + word + \" \", \" \" + dic[word] + \" \")\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2jF1z6ADwDkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train['text'] = train['text'].apply(lambda x:mapping_replacer(x,dic))\n",
        "train['selected_text'] = train['selected_text'].apply(lambda x:mapping_replacer(x,dic))\n",
        "test['text'] = test['text'].apply(lambda x:mapping_replacer(x,dic))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oQvXJjA2wDkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13_kuvF2wDkj",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3.3\"></a>\n",
        "## 3.3 Spelling corrector\n",
        "\n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HGXgY-uEwDkj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "misspell_data = pd.read_csv(\"/kaggle/input/spelling/aspell.txt\",sep=\":\",names=[\"correction\",\"misspell\"])\n",
        "misspell_data.misspell = misspell_data.misspell.str.strip()\n",
        "misspell_data.misspell = misspell_data.misspell.str.split(\" \")\n",
        "misspell_data = misspell_data.explode(\"misspell\").reset_index(drop=True)\n",
        "misspell_data.drop_duplicates(\"misspell\",inplace=True)\n",
        "miss_corr = dict(zip(misspell_data.misspell, misspell_data.correction))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "G-84jP3YwDkm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def misspelled_correction(val):\n",
        "    for x in val.split(): \n",
        "        if x in miss_corr.keys(): \n",
        "            val = val.replace(x, miss_corr[x]) \n",
        "    return val\n",
        "\n",
        "train[\"text\"] = train[\"text\"].apply(lambda x : misspelled_correction(x))\n",
        "test[\"text\"] = test[\"text\"].apply(lambda x : misspelled_correction(x))\n",
        "train[\"selected_text\"] = train['selected_text'].apply(lambda x : misspelled_correction(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Bi2P8rXwDkn",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3.4\"></a>\n",
        "## 3.4 Cleaning Corpus \n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "Preprocessig is the process which cleans the data to get all in a consistent format and  accurate prediction score. So this process is directly connected with the model result. We need to clean,tokenize and convert data into the matric form \n",
        "\n",
        "* make a lowercase\n",
        "* remove text in square brackets\n",
        "* reomve hyperlin,HTML\n",
        "* remove punctuation\n",
        "* remove line change\n",
        "* remove words containig numbers\n",
        "\n",
        "For that we need special libraries "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiJeVNVjwDkn",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3.5\"></a>\n",
        "## 3.5 Tokenization\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens. I'll show 4 famous Tokenization methods\n",
        "\n",
        "<div align='left'><font size=\"3\" color=\"#51ABDA\">Wordtokenizer</font></div>\n",
        "split the token by white space\n",
        "\n",
        "<div align='left'><font size=\"3\" color=\"#51ABDA\">Treebanktokenizer</font></div>\n",
        "\n",
        "- split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
        "\n",
        "- treat most punctuation characters as separate tokens\n",
        "\n",
        "- split off commas and single quotes, when followed by whitespace\n",
        "\n",
        "- separate periods that appear at the end of line\n",
        "\n",
        "<div align='left'><font size=\"3\" color=\"#51ABDA\">WordPunctTokenizer</font></div>\n",
        "split the token by punctuation\n",
        "\n",
        "<div align='left'><font size=\"3\" color=\"#51ABDA\">RegexpTokenizer</font></div>\n",
        "A tokenizer that splits a string using a regular expression, which matches either the tokens or the separators between tokens.\n",
        "\n",
        "<hr>\n",
        "In this we'll use `RegexpTokenizer`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xuvmR6X3wDko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cEXBIsL7wDkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "########################### Cleaning Corpus ####################################\n",
        "\n",
        "def clean_text(text):\n",
        "    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n",
        "    and remove words containing numbers.'''\n",
        "    text = text.lower()\n",
        "    # make a lowercase \n",
        "\n",
        "    # re.sub(pattern,repl,string) \n",
        "    # find corresponded pattern from string convert to repl\n",
        "    \n",
        "    \n",
        "    text = re.sub('\\[.*?\\]', '', text)\n",
        "    #remove text in square brackets\n",
        "    \n",
        "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub('<.*?>+', '', text)\n",
        "    # remove Hyperlink,HTML \n",
        "    \n",
        "    \n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
        "    # remove punctuation\n",
        "    \n",
        "    text = re.sub('\\n', '', text)\n",
        "    # remove line change\n",
        "\n",
        "    text = re.sub('\\w*\\d\\w*', '', text)\n",
        "    # remove words containing numbers.\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "########################### Tokenization ####################################\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    Cleaning and parsing the text.\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
        "    nopunc = clean_text(text)\n",
        "    tokenized_text = tokenizer.tokenize(nopunc)\n",
        "    combined_text = ' '.join(tokenized_text)\n",
        "    return combined_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oBnXiQcwwDkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Applying the cleaning function to both test and training datasets\n",
        "train['text'] = train['text'].apply(str).apply(lambda x: text_preprocessing(x))\n",
        "test['text'] = test['text'].apply(str).apply(lambda x: text_preprocessing(x))\n",
        "train['selected_text'] = train['selected_text'].apply(str).apply(lambda x: text_preprocessing(x))\n",
        "\n",
        "\n",
        "# Let's take a look at the updated text\n",
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "6u2JWFGjwDkt",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec3.6\"></a>\n",
        "## 3.6 Removing Stopwords\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "\n",
        "- This part can be belonged to the `Part 4.1`.But personally I want to show it separately because for removing stopwords we need to use special libraires.\n",
        "\n",
        "- Stopword is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
        "\n",
        "- The reason for ignoring is that those words are not cruial component of the **sentiment analysis**. In other words stopwords have no meaning ,which is related to sentiment or expression\n",
        "\n",
        "\n",
        "**Notice** \n",
        "\n",
        "In order to remve stopswords we have to split word by word. If you compile function without spliting words. your text table will be splited by alphabet.So don't forget to do it !!! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "frCBFRASwDku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1 split word by word\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "def tokenizer(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    return word_tokens\n",
        "\n",
        "'''\n",
        "Alternative of def tokenizer(text)\n",
        "\n",
        "# from collections import Counter\n",
        "# train['word'] = train['selected_text'].apply(lambda x:str(x).split())\n",
        "'''\n",
        "\n",
        "train['word from ST'] = train['selected_text'].apply(str).apply(lambda x: tokenizer(x))\n",
        "train['word from T'] = train['text'].apply(str).apply(lambda x: tokenizer(x))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5Un0WXHjwDkx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2 remove stopwords \n",
        "\n",
        "def remove_stopword(x):\n",
        "    return [y for y in x if y not in stopwords.words('english')]\n",
        "train['word from ST'] = train['word from ST'].apply(lambda x:remove_stopword(x))\n",
        "train['word from T'] = train['word from T'].apply(lambda x: remove_stopword(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wfzfYefAwDkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDjet7PawDk3",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "\n",
        "> Now we are ready for Word data Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "KFkHrNCywDk4",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec4\"></a>\n",
        "# 4. Word data Analysis\n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "hmwa9v9ewDk5",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec4.1\"></a>\n",
        "## 4.1 Word Frequencey \n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "nLKy_zNRwDk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "5yxY0sMdwDk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counting = Counter([item for sublist in train['word from T'] for item in sublist])\n",
        "counting_table = pd.DataFrame(counting.most_common(30))\n",
        "counting_table.drop([0],inplace=True)\n",
        "counting_table.columns = ['word','counting']\n",
        "\n",
        "plt.figure(figsize=(17,10))\n",
        "ax= sns.barplot(data=counting_table,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 words from text '.title(),fontsize=20)\n",
        "\n",
        "ax.set_ylabel('Word counting',fontsize=15)\n",
        "ax.set_xlabel('Top 30 words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BO-wnPV5wDlB",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "> `day` >> `good` >> `get` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "wBX2kGFxwDlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counting = Counter([item for sublist in train['word from ST'] for item in sublist])\n",
        "counting_table = pd.DataFrame(counting.most_common(30))\n",
        "counting_table.drop([0],inplace=True)\n",
        "counting_table.columns = ['word','counting']\n",
        "\n",
        "plt.figure(figsize=(17,10))\n",
        "ax= sns.barplot(data=counting_table,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_ylabel('Word counting',fontsize=15)\n",
        "ax.set_xlabel('Top 30 words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKBFp4OIwDlE",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        ">`good` >> `day` >> `love` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Y-LYAS5bwDlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive = train[train['sentiment']=='positive']\n",
        "negative = train[train['sentiment']=='negative']\n",
        "neutral = train[train['sentiment']=='neutral']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "14ZjjurmwDlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counting_positive_ST = Counter([item for sublist in positive['word from ST'] for item in sublist])\n",
        "counting_positive_ST = pd.DataFrame(counting_positive_ST.most_common(30))\n",
        "counting_positive_ST.drop([0],inplace=True)\n",
        "counting_positive_ST.columns = ['word','counting']\n",
        "\n",
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=counting_positive_ST,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 positive words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_ylabel('Word counting',fontsize=15)\n",
        "ax.set_xlabel('Top 30 positive words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idprValdwDlK",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "> `happy` >> `love` >> `day`..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "l3QOOwXCwDlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counting_negative_ST = Counter([item for sublist in negative['word from ST'] for item in sublist])\n",
        "counting_negative_ST = pd.DataFrame(counting_negative_ST.most_common(30))\n",
        "counting_negative_ST.drop([0],inplace=True)\n",
        "counting_negative_ST.columns = ['word','counting']\n",
        "\n",
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=counting_negative_ST,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 negative words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_ylabel('Word counting',fontsize=15)\n",
        "ax.set_xlabel('Top 30 negative words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f7jj8mEwDlM",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "> `miss` >> `sad` >> `sorry` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "uVEsSNmwwDlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "counting_neutral_ST = Counter([item for sublist in neutral['word from ST'] for item in sublist])\n",
        "counting_neutral_ST = pd.DataFrame(counting_neutral_ST.most_common(30))\n",
        "counting_neutral_ST.drop([0],inplace=True)\n",
        "counting_neutral_ST.columns = ['word','counting']\n",
        "\n",
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=counting_neutral_ST,x='word',y='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 neutral words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_ylabel('Word counting',fontsize=15)\n",
        "ax.set_xlabel('Top 30 neutral words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSVagSuBwDlO",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "> `get` >> `go` >> `got`..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "id": "w_hrtLk8wDlO",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec4.2\"></a>\n",
        "## 4.2 Word Cloud\n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "g8swtO0CwDlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_text = train[train['sentiment']=='positive']['text']\n",
        "negative_text = train[train['sentiment']=='negative']['text']\n",
        "neutral_text = train[train['sentiment']=='neutral']['text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "kQEXVRKhwDlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "from PIL import Image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "RQAFPxWJwDlS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=[30, 15])\n",
        "wordcloud1 = WordCloud( background_color='white',\n",
        "                        width=1500,\n",
        "                        height=800).generate(\" \".join(positive_text))\n",
        "ax1.imshow(wordcloud1)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('Positive text',fontsize=20);\n",
        "\n",
        "wordcloud2 = WordCloud( background_color='white',\n",
        "                        width=1500,\n",
        "                        height=800).generate(\" \".join(negative_text))\n",
        "ax2.imshow(wordcloud2)\n",
        "ax2.axis('off')\n",
        "ax2.set_title('Negative text',fontsize=20);\n",
        "\n",
        "wordcloud3 = WordCloud( background_color='white',\n",
        "                        width=1500,\n",
        "                        height=800).generate(\" \".join(neutral_text))\n",
        "ax3.imshow(wordcloud3)\n",
        "ax3.axis('off')\n",
        "ax3.set_title('Neutral text',fontsize=20);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "2a219yE2wDlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(15.0,8.0), color = 'white',\n",
        "                   title = None, title_size=40, image_color=False):\n",
        "    wordcloud = WordCloud(background_color=color,\n",
        "                    max_words = max_words,\n",
        "                    max_font_size = max_font_size, \n",
        "                    random_state = 42,\n",
        "                    width=400, \n",
        "                    height=200,\n",
        "                    mask = mask)\n",
        "    wordcloud.generate(str(text))\n",
        "    \n",
        "    plt.figure(figsize=figure_size)\n",
        "    if image_color:\n",
        "        image_colors = ImageColorGenerator(mask);\n",
        "        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n",
        "        plt.title(title, fontdict={'size': title_size,  \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    else:\n",
        "        plt.imshow(wordcloud);\n",
        "        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n",
        "                                  'verticalalignment': 'bottom'})\n",
        "    plt.axis('off');\n",
        "    plt.tight_layout()  \n",
        "d = '/kaggle/input/masks/masks-wordclouds/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "fK3CvVpawDlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_selected_text = train[train['sentiment']=='positive']['selected_text']\n",
        "negative_selected_text = train[train['sentiment']=='negative']['selected_text']\n",
        "neutral_selected_text = train[train['sentiment']=='neutral']['selected_text']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Qjjd9eDswDlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_mask = np.array(Image.open(d+ 'comment.png'))\n",
        "plot_wordcloud(positive_selected_text,mask=pos_mask,color='white',max_font_size=70,title_size=20,title=\" Positive selected text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "8bozDbh5wDlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_mask = np.array(Image.open(d+ 'loc.png'))\n",
        "plot_wordcloud(negative_selected_text,mask=pos_mask,color='white',max_font_size=70,title_size=20,title=\"Negative selected text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "k95EBO0owDlb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_mask = np.array(Image.open(d+ 'star.png'))\n",
        "plot_wordcloud(neutral_selected_text,mask=pos_mask,color='white',max_font_size=70,title_size=20,title=\"Neutral selected text\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw8W4-CVwDle",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec5\"></a>\n",
        "# 5. N-gram analysis\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "I think, a thousand  hearing are not worth one seeing. Chech out what is **Unigram(1-grams)**,**Bigram(2-gram)** and **Trigrams(3-gram)**\n",
        "[Source](https://deepai.org/machine-learning-glossary-and-terms/n-gram)\n",
        "![n%20gram.JPG](attachment:n%20gram.JPG)\n",
        "N-grams are simply all combinations of words or letters of length n that you can find in your source text.\n",
        "\n",
        "I won't do Unigram because it's actually same as the tokenization of text and visualize it (Chapter 4 : Word data analysis)\n",
        "\n",
        "For N-gram analysis we need countvectorizer from sci-kit learn [How to list the most common words from text corpus using countvectorizer ](https://medium.com/@cristhianboujon/how-to-list-the-most-common-words-from-text-corpus-using-scikit-learn-dad4d0cab41d)\n",
        "\n",
        "**Notice** \n",
        "In the part 4 We used Counter library to get a common word, but  this function is incapable to get a bigram or trigram. In other word that can get only unigram. That why we use another function "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "SV5oa6UkwDlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# How to list the most common words from text corpus using countvectorizer \n",
        "def WordRanking(corpus,n_gram,n=None):\n",
        "   \n",
        "    vec = CountVectorizer(ngram_range=n_gram,stop_words = 'english').fit(corpus)\n",
        "    # Here we get a Bag of Word model \n",
        "    \n",
        "    bag_of_words = vec.transform(corpus)\n",
        "    # bag_of_words a matrix where each row represents a specific text in corpus and each column represents a word in vocabulary,\n",
        "    # that is, all words found in corpus. Note that bag_of_words[i,j] is the occurrence of word j in the text i.\n",
        "    \n",
        "    sum_words = bag_of_words.sum(axis=0) \n",
        "    # we are adding the elements for each column of bag_of_words matrix.\n",
        "    \n",
        "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
        "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
        "    #Finally we sort a list of tuples that contain the word and their occurrence in the corpus.\n",
        "    \n",
        "    return words_freq[:n]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "p3sQ4t8WwDlh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "postive_bigrams = WordRanking(positive_selected_text,(2,2),30)\n",
        "negative_bigrams = WordRanking(negative_selected_text,(2,2),30)\n",
        "neutral_bigrams = WordRanking(neutral_selected_text,(2,2),30)\n",
        "\n",
        "postive_bigrams = pd.DataFrame(postive_bigrams,columns=['word','counting'])\n",
        "negative_bigrams = pd.DataFrame(negative_bigrams,columns=['word','counting'])\n",
        "neutral_bigrams = pd.DataFrame(neutral_bigrams,columns=['word','counting'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwDTsYkLwDlj",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec5.1\"></a>\n",
        "## 5.1 Bigrams analysis\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "cjeYJjBJwDlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=postive_bigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 positive bigram words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_ylabel('Word counting',fontsize=15)\n",
        "ax.set_xlabel('Top 30 positive bigram words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJK_7vg0wDll",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> `mothers day` >> `happy mothers` >> `good morning` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "J-s2UZiIwDll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=negative_bigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 negative bigram words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Word counting',fontsize=15)\n",
        "ax.set_ylabel('Top 30 negative bigram words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQWnw2BYwDln",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "\n",
        "> `im sorry` >> `dont like` >> `feel like` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "oiaku8XkwDln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=neutral_bigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 neutral bigram words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Word counting',fontsize=15)\n",
        "ax.set_ylabel('Top 30 neutral bigram words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGMQqmjYwDlr",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec5.2\"></a>\n",
        "## 5.2 Trigrams analysis\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "PXKk3IK5wDlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_trigrams = WordRanking(positive_selected_text,(3,3),30)\n",
        "negative_trigrams = WordRanking(negative_selected_text,(3,3),30)\n",
        "neutral_trigrams = WordRanking(neutral_selected_text,(3,3),30)\n",
        "\n",
        "positive_trigrams = pd.DataFrame(positive_trigrams,columns=['word','counting'])\n",
        "negative_trigrams = pd.DataFrame(negative_trigrams,columns=['word','counting'])\n",
        "neutral_trigrams = pd.DataFrame(neutral_trigrams,columns=['word','counting'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "m7ueDWrtwDlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=positive_trigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 positive trigram words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Word counting',fontsize=15)\n",
        "ax.set_ylabel('Top 30 positive trigram words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1aAOu8HwDlx",
        "colab_type": "text"
      },
      "source": [
        "Observation\n",
        "\n",
        "> `happy mothers day` >> `happy star wars` >> `star wars day` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "hU1ydXMVwDlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=negative_trigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 negative trigram words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Word counting',fontsize=15)\n",
        "ax.set_ylabel('Top 30 negative trigram words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ST5AqgDTwDlz",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> `dont feel good` >> `dont think im` >> `hate hate hate` ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "UNL_PwCswDl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(19,10))\n",
        "ax= sns.barplot(data=neutral_trigrams,y='word',x='counting',facecolor=(1, 1, 1, 0),edgecolor='black')\n",
        "ax.set_title('Top 30 neutral trigram words from selected text'.title(),fontsize=20)\n",
        "\n",
        "ax.set_xlabel('Word counting',fontsize=15)\n",
        "ax.set_ylabel('Top 30 neutral trigram words',fontsize=15);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8yTkSSrwDl3",
        "colab_type": "text"
      },
      "source": [
        "Observation \n",
        "\n",
        "> `just got home` >> `happy mothers day` >>  `star wars day`..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYHFylDBwDl5",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec6\"></a>\n",
        "# 6 Options \n",
        "<a href=\"#toc\">Back to the structure</a>\n",
        "\n",
        "<a name=\"sec6.1\"></a>\n",
        "## 6.1 Spelling corrector\n",
        "\n",
        "There is another concept of spellig corrector.[How to Write a Spelling Corrector from Peter Norvig](http://norvig.com/spell-correct.html).If you want to use this corrector, make sure that you have 'big.txt' file on your kernel. The reason why I didn't use this corrector is that my laptop can't reach the ability to compile the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3Ms6QwDKwDl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(WORDS.values())): \n",
        "    \"Probability of `word`.\"\n",
        "    return WORDS[word] / N\n",
        "\n",
        "def correction(word): \n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidates(word), key=P)\n",
        "\n",
        "def candidates(word): \n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
        "\n",
        "def known(words): \n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in WORDS)\n",
        "\n",
        "def edits1(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2(word): \n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
        "    \n",
        "    \n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ja8U3eEBwDl7",
        "colab_type": "text"
      },
      "source": [
        "<a name=\"sec6.2\"></a>\n",
        "## 6.2 Removig weird spaces\n",
        "<a href=\"#toc\">(Back to the structure)</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "dsHD52N2wDl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_space(text): \n",
        "    text = text.strip() \n",
        "    text = text.split()\n",
        "    return \" \".join(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EcWf-43wDl9",
        "colab_type": "text"
      },
      "source": [
        "**Notice**\n",
        "\n",
        "As I mentioned before this code is not working because `big.txt` isn't in the repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_6yArmwDl-",
        "colab_type": "text"
      },
      "source": [
        "<a id='sec7'></a>\n",
        "# 7. Closing \n",
        "\n",
        "<a href=\"#toc\">(Back to the structure)</a>\n",
        "\n",
        "Thanks for your reading my kernal.Because I'm kaggle newcomer i need much more time to practice in order to improve my kernal.\n",
        "In the next time I'll make a second part of `Tweet sentiment` which is modeling and prediction of NLP.\n",
        "\n",
        "### thank you all very much :) \n",
        "\n"
      ]
    }
  ]
}