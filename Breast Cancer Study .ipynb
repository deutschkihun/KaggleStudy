{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set(style=\"darkgrid\")\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\n# Machine learning libraries that I'll use in this study \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,cross_val_score,cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction \n\n Breast cancer is cancer that forms in the cells of the breasts.\n\n After skin cancer, breast cancer is the most common cancer diagnosed in women in the United States. Breast cancer can occur in both men and women, but it's far more common in women.Substantial support for breast cancer awareness and research funding has helped created advances in the diagnosis and treatment of breast cancer. \n\nBreast cancer survival rates have increased, and the number of deaths associated with this disease is steadily declining, largely due to factors such as earlier detection, a new personalized approach to treatment and a better understanding of the disease.\n\nIn this study you'll get a csv file called 'breast-cancer-wisconsin-data'.From there you will get informations about diagnosis and specific data in numeric value.Our goal of this study is prediction of the breast cancer whether they have **benign or malignant** by using factor columns "},{"metadata":{},"cell_type":"markdown","source":"# Data Wrangling"},{"metadata":{},"cell_type":"markdown","source":"## 1. Gathering  "},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load csv file\ndf = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## 2. Assessing & Cleaning  "},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n# load 5 rows of df \n# There are a few unnecessary columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['id','Unnamed: 32'],axis=1,inplace=True)\n#In this study 'id' and 'Unnamed: 32' are not needed \n#So drop both columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()\n# checking missing value \n# No missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()\n# get a information about each column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape\n# rows and columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.columns\n# all columns in df ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> We have total 31 columns which consist of 1 categorical and 30 quantitative values. \nSo we gonna use `diagonisis` for final result.That means using different kind of cancer factors(30 quantitative values) we will get `diagonisis` prediction. "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Of course that's really good if we use all factor columns to guess who has probably have a different type of `diagnosis`.But it takes a lot of time. Also it's really **hard to read at a glance**.So what we`re going to do now is copy the original file and filter columns that I want to use. \n\n**Notice**\n- Copying the file is also important process.Because keeping the original file is more easiler when you use that file again later. \n\n**Columns information**\n- mean : average\n- se(standard error) : quantifies the variation in the means from multiple sets of measurements.In other words standard error is the mean of standard deviation.\n- standard deviation : quantifies the variation within a set of measurements \n- worst : worst or largest mean value from each data\n\nThe confusing things between **standard error** and **standard deviation** is that the standard error can be estimated from a single set of measurements, even though it describes the means from multiple sets. Thus,even if you only have a singel set of measurements, you are often given the option to plot the standard error. "},{"metadata":{},"cell_type":"markdown","source":"## What is/are the main feature(s) of interest in your dataset?\n- diagnosis \n\n## What features in the dataset do you think will help support your investigation into your feature(s) of interest?\n\n- all the other columns  \n\n## Unnecessary featues in the dataset do you think\n- Nothings, we will use all columns for factors of breast cancer "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data Visualization "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 1. Univariate Exploration of data\n\nUsing only one variable to visualize **df_new table**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#countplot\nplt.subplots(figsize=(10,5))\nsns.countplot(data=df,x='diagnosis');\n\nplt.title('Diagnosis counting'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('Type of diagonosis'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('Count'.title(),\n           fontsize=14,weight=\"bold\")\n\nplt.legend(['malignant','benign'],loc='center right',bbox_to_anchor=(1.2, 0.93), \n           title=\"Diagonisis\", title_fontsize = 14);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> There are a lot more benign than malignant.So we called `inbalanced data`.This is actually not a extreme case. If it`s too strong.We should have a balance between two type of diagnosis in order to get a right prediction later."},{"metadata":{"trusted":true},"cell_type":"code","source":"#pie chart  \n\nplt.figure(figsize=(15,7))\nsorted_counts = df['diagnosis'].value_counts()\n# count the value of diagnosis \nax=plt.pie(sorted_counts, labels = sorted_counts.index, startangle = 90,\n        counterclock = False,pctdistance=0.8 ,wedgeprops = {'width' : 0.4}, autopct='%1.0f%%');\n\n\nplt.title('Proprotion of malignant and benign'.title(),\n         fontsize = 14, weight=\"bold\");\n\nplt.legend(['Benign(B)','Malignant(M)'],bbox_to_anchor=(1,0.9));","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"> As you can see on the pie chart benign possesses 63% of the total dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#distplot = histogram + curveline \n# for example : radius mean\n\nplt.subplots(figsize=(15,7))\nx = df.radius_mean\nbins = np.arange(0,30,1)\nsns.distplot(x,bins=bins,color='black')\n\n\nplt.title('radius mean Histogram'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('radius mean range'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('Count in percentage'.title(),\n           fontsize=14,weight=\"bold\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Approximately normal distributed graph"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,7))\nx = df.symmetry_mean\nbins = np.arange(0,1,0.01)\nsns.distplot(x,color='black',bins=bins)\n\n\nplt.title('Symmetry mean Histogram'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('symmetry mean range'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('Count'.title(),\n           fontsize=14,weight=\"bold\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Nornmal distributed and symmetry but not the meam value isn't located on the center. "},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.subplots(figsize=(15,7))\nx = df.concavity_mean\nbins = np.arange(0,1,0.01)\nsns.distplot(x,color='black',bins=bins)\n\n\nplt.title('concavity mean Histogram'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('concavity mean range'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('Count'.title(),\n           fontsize=14,weight=\"bold\");","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Right skewed normal distributed "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 2.Bivariate Exploration of Data\n\nUsing only one variable to visualize **df_new** table"},{"metadata":{"trusted":true},"cell_type":"code","source":"# split table into different valriables \ny=df.diagnosis \nx = df.iloc[:,1:] \n\n# standardization\nstand = (x - x.mean()) / (x.std())             ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> Why we're doing standardization? because columns with **mean,se and worst** have different size of value.So it's hard to compare with raw data.That's why we're doing standardization to make it comparable.\n\n\n## $$ z_{score} = \\frac {(x- \\mu)}{\\sigma}$$\n\n\n- Z = standard score or z score , this score tells us you how many standard deviations from the mean your score is.\n- x = observed value\n- $\\mu$ = mean value of dataset\n- $\\sigma$ = standard deviation of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Because we have 30 sub features we'll divide 3 groups to visualize\n\ndata = pd.concat([y,stand.iloc[:,0:10]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# In order to visualize different type of numeric value in one graph.We're going to melt df_new table into the new table called `data`.\n# id_var : Column(s) to use as identifier variables.\n# var_name : Name to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’.\n# value_name : Name to use for the ‘value’ column. \n\n\nplt.figure(figsize=(15,7))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n\n\nplt.title('Sub features with standardization(first 10 features)'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('Sub features'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('z score'.title(),\n           fontsize=14,weight=\"bold\");\n\nplt.xticks(rotation=45);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([y,stand.iloc[:,10:20]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# In order to visualize different type of numeric value in one graph.We're going to melt df_new table into the new table called `data`.\n# id_var : Column(s) to use as identifier variables.\n# var_name : Name to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’.\n# value_name : Name to use for the ‘value’ column. \n\n\nplt.figure(figsize=(15,7))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n\n\nplt.title('Sub features with standardization(Second 10 features)'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('Sub features'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('z score'.title(),\n           fontsize=14,weight=\"bold\");\n\nplt.xticks(rotation=45);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.concat([y,stand.iloc[:,20:31]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\n\n# In order to visualize different type of numeric value in one graph.We're going to melt df_new table into the new table called `data`.\n# id_var : Column(s) to use as identifier variables.\n# var_name : Name to use for the ‘variable’ column. If None it uses frame.columns.name or ‘variable’.\n# value_name : Name to use for the ‘value’ column. \n\n\nplt.figure(figsize=(15,7))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\n\n\nplt.title('Sub features with standardization(last 10 features)'.title(),\n         fontsize = 14, weight=\"bold\")\n\nplt.xlabel('Sub features'.title(),\n          fontsize=14,weight=\"bold\")\n\nplt.ylabel('z score'.title(),\n           fontsize=14,weight=\"bold\");\n\nplt.xticks(rotation=45);\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## find out proper plotting ....","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"### 3.Multivariate Exploration of Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Pargrid\n# Facetgrid\n# Scatterplot with size or shape  ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction "},{"metadata":{},"cell_type":"markdown","source":"> In this lat part we'll do a prediction.For that we'll use **SVM,RandomForest,DecisionTree,KNN,LogisticRegression** which are the most popular and fundamental machine learning algorithms in Data science. If you don't have any idea of it, please check how work each of algorithms above before you dive into the last part of this study"},{"metadata":{},"cell_type":"markdown","source":"### Split Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = train_test_split(df,test_size=0.2,random_state=2019)\n# test size =0.2 means I will use 20% for testing \n# so that means we use 80% for training. Spliting teat set and training set is very important.\n# Spliting test-set and training-set is very important.Because we have to use tes-set to examine our prediction model and get a performance in numeric value.\n#So never use test-set for training.Otherwise we can't get a exact result of prediction model.\n# Reason why we use random_state : https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn\n\nx_train = train.drop(['diagnosis'],axis=1)\ny_train = train.diagnosis\n\n# we should think about why we drop diagonosis column.Because we want to know the diagnosis in the end (That mean malignant or benign)\n# We're going to use other columns as a x variable to get a diagonosis(y variable).That's the reason why we drop diagnosis in x_train and x_test\n\nx_test = test.drop(['diagnosis'],axis=1)\ny_test = test.diagnosis \n\nprint(len(train),len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got 455 rows for trainig and 114 rows for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"### SVM ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\nmodel.fit(x_train,y_train)\n# learning train dataset\n\ny_pred = model.predict(x_test)\n# prediction test dataset\n\nprint('SVM: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n# metrics.accuracy_score : measure the accurace_score\n# so we compare prediction of y (prediction, y_pred) and test result of y (fact,y_test) how close our y_pred to y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"So we got 91.23%.That means our prediction is 91.23% equal to y_test result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('DecisionTreeClassifier: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier()\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('KNeighborsClassifier: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(solver='lbfgs',max_iter=2000)\n# about parameters: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('LogisticRegression: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('RandomForestClassifier: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Compute Feature Importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.Series(\n     model.feature_importances_,\n    index=x_train.columns).sort_values(ascending=False)\n\n# model.feature_importances_ shows which paramet is important to predict the model \n# we are matching train dataset columns with model.feature_importances and saved in pandas series as a numeric values \nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Extract Top 5 Features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_features = features.keys()[:5]\n# series.keys() : this function is an alias for index. It returns the index labels of the given series object.\n\nprint(top_5_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### SVM(Top 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\nmodel.fit(x_train[top_5_features],y_train)\n\ny_pred = model.predict(x_test[top_5_features])\n# prediction test dataset\n\nprint('SVM(Top5): %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cross Validation (principle version)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\n\ncv = KFold(n_splits=5,random_state=2019)\n# Interation : K=5\n\naccs = []\n\nfor train_index,test_index in cv.split(df[top_5_features]):\n    x_train = df.iloc[train_index][top_5_features]\n    y_train = df.iloc[train_index].diagnosis\n    \n    x_test = df.iloc[test_index][top_5_features]\n    y_test = df.iloc[test_index].diagnosis\n    \n    \n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    accs.append(metrics.accuracy_score(y_pred,y_test))\n    # position of y_pred and y_test are not important\n    \nprint(accs)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cross Validation (simple version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\ncv = KFold(n_splits=5,random_state=2019)\n\naccs = cross_val_score(model,df[top_5_features],df.diagnosis,cv=cv)\n# cross_vall_score : apply cross validation (in our case would be KFold) and learning.\n# In the end will be print out the model score\n# x variable : df[top_5_features] , y variable : di.diagnosis\nprint(accs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Test all Models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = {\n    'SVM': svm.SVC(gamma='scale'),\n    'DecisionTreeClassifier':DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'LogisticRegression': LogisticRegression(solver='lbfgs',max_iter=2000),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n    \n}\n\ncv = KFold(n_splits=5,random_state=2019)\n\nfor name, model in model.items():\n    scores = cross_val_score(model,df[top_5_features],df.diagnosis,cv=cv)\n    \n    print('%s:%.2f%%' % (name,np.mean(scores)*100))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Normalized Dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1))\n# scale the range between 0 and 1 \nscaled_data = scaler.fit_transform(df[top_5_features])\n\nmodel = {\n    'SVM': svm.SVC(gamma='scale'),\n    'DecisionTreeClassifier':DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'LogisticRegression': LogisticRegression(solver='lbfgs',max_iter=2000),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n    \n}\n\ncv = KFold(n_splits=5,random_state=2019)\n\nfor name, model in model.items():\n    scores = cross_val_score(model,scaled_data,df.diagnosis,cv=cv)\n    \n    print('%s:%.2f%%' % (name,np.mean(scores)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}