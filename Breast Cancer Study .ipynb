{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set(style=\"darkgrid\")\nimport warnings\nwarnings.filterwarnings(\"ignore\") \n\n# Machine learning libraries that I'll use in this study \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold,cross_val_score,cross_val_predict\nfrom sklearn import metrics\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing for Data wrangling"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load csv file\ndf = pd.read_csv(\"/kaggle/input/breast-cancer-wisconsin-data/data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()\n# load 5 rows of df \n# There are a few unnecessary columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop(['id','Unnamed: 32'],axis=1,inplace=True)\n#In this study 'id' and 'Unnamed: 32' are not needed \n#So drop both columns ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isna().sum()\n# checking missing value \n# No missing values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.info()\n# get a information about each column","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Data Wrangling "},{"metadata":{},"cell_type":"markdown","source":"## Gathering , Assessing, cleaning "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction "},{"metadata":{},"cell_type":"markdown","source":"### Split Train and Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train,test = train_test_split(df,test_size=0.2,random_state=2019)\n# test size =0.2 means I will use 20% for testing \n# so that means we use 80% for training. Spliting teat set and training set is very important.\n# Spliting test-set and training-set is very important.Because we have to use tes-set to examine our prediction model and get a performance in numeric value.\n#So never use test-set for training.Otherwise we can't get a exact result of prediction model.\n# Reason why we use random_state : https://stackoverflow.com/questions/28064634/random-state-pseudo-random-number-in-scikit-learn\n\nx_train = train.drop(['diagnosis'],axis=1)\ny_train = train.diagnosis\n\n# we should think about why we drop diagonosis column.Because we want to know the diagnosis in the end (That mean malignant or benign)\n# We're going to use other columns as a x variable to get a diagonosis(y variable).That's the reason why we drop diagnosis in x_train and x_test\n\nx_test = test.drop(['diagnosis'],axis=1)\ny_test = test.diagnosis \n\nprint(len(train),len(test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We got 455 rows for trainig and 114 rows for testing"},{"metadata":{"trusted":true},"cell_type":"code","source":"### SVM ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\nmodel.fit(x_train,y_train)\n# learning train dataset\n\ny_pred = model.predict(x_test)\n# prediction test dataset\n\nprint('SVM: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n# metrics.accuracy_score : measure the accurace_score\n# so we compare prediction of y (prediction, y_pred) and test result of y (fact,y_test) how close our y_pred to y_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"So we got 91.23%.That means our prediction is 91.23% equal to y_test result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### DecisionTreeClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = DecisionTreeClassifier()\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('DecisionTreeClassifier: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### KNeighborsClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = KNeighborsClassifier()\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('KNeighborsClassifier: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### LogisticRegression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = LogisticRegression(solver='lbfgs',max_iter=2000)\n# about parameters: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('LogisticRegression: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### RandomForestClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(x_train,y_train)\n\ny_pred = model.predict(x_test)\n\nprint('RandomForestClassifier: %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Compute Feature Importances","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"features = pd.Series(\n     model.feature_importances_,\n    index=x_train.columns).sort_values(ascending=False)\n\n# model.feature_importances_ shows which paramet is important to predict the model \n# we are matching train dataset columns with model.feature_importances and saved in pandas series as a numeric values \nprint(features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Extract Top 5 Features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"top_5_features = features.keys()[:5]\n# series.keys() : this function is an alias for index. It returns the index labels of the given series object.\n\nprint(top_5_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### SVM(Top 5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\nmodel.fit(x_train[top_5_features],y_train)\n\ny_pred = model.predict(x_test[top_5_features])\n# prediction test dataset\n\nprint('SVM(Top5): %.2f' % (metrics.accuracy_score(y_pred,y_test)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cross Validation (principle version)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\n\ncv = KFold(n_splits=5,random_state=2019)\n# Interation : K=5\n\naccs = []\n\nfor train_index,test_index in cv.split(df[top_5_features]):\n    x_train = df.iloc[train_index][top_5_features]\n    y_train = df.iloc[train_index].diagnosis\n    \n    x_test = df.iloc[test_index][top_5_features]\n    y_test = df.iloc[test_index].diagnosis\n    \n    \n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    accs.append(metrics.accuracy_score(y_pred,y_test))\n    # position of y_pred and y_test are not important\n    \nprint(accs)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Cross Validation (simple version)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = svm.SVC(gamma='scale')\ncv = KFold(n_splits=5,random_state=2019)\n\naccs = cross_val_score(model,df[top_5_features],df.diagnosis,cv=cv)\n# cross_vall_score : apply cross validation (in our case would be KFold) and learning.\n# In the end will be print out the model score\n# x variable : df[top_5_features] , y variable : di.diagnosis\nprint(accs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Test all Models","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = {\n    'SVM': svm.SVC(gamma='scale'),\n    'DecisionTreeClassifier':DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'LogisticRegression': LogisticRegression(solver='lbfgs',max_iter=2000),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n    \n}\n\ncv = KFold(n_splits=5,random_state=2019)\n\nfor name, model in model.items():\n    scores = cross_val_score(model,df[top_5_features],df.diagnosis,cv=cv)\n    \n    print('%s:%.2f%%' % (name,np.mean(scores)*100))\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Normalized Dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1))\n# scale the range between 0 and 1 \nscaled_data = scaler.fit_transform(df[top_5_features])\n\nmodel = {\n    'SVM': svm.SVC(gamma='scale'),\n    'DecisionTreeClassifier':DecisionTreeClassifier(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'LogisticRegression': LogisticRegression(solver='lbfgs',max_iter=2000),\n    'RandomForestClassifier': RandomForestClassifier(n_estimators=100)\n    \n}\n\ncv = KFold(n_splits=5,random_state=2019)\n\nfor name, model in model.items():\n    scores = cross_val_score(model,scaled_data,df.diagnosis,cv=cv)\n    \n    print('%s:%.2f%%' % (name,np.mean(scores)*100))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}